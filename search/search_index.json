{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p>Welcome to the Property Boundaries Service documentaion!</p> <p>Here are some things you should know as a new developer.</p>"},{"location":"#how-is-the-pbs-used","title":"How is the PBS used?","text":"<p>The PBS serves data to the LandExplorer app's 'Land Ownership' layers. The LX User guide explains how to use the layer in more detail.</p>"},{"location":"#where-does-the-property-data-come-from","title":"Where does the property data come from?","text":"<p>To get an overview of the data we handle, and how it gets updated by the Ownerships + INSPIRE pipeline, see pipeline.md.</p>"},{"location":"#useful-dev-commands","title":"Useful dev commands","text":"<ul> <li><code>npx sequelize-cli db:migrate</code> to run new database migration(s)</li> <li><code>npx sequelize-cli db:migrate:undo:all</code> to reset database migrations</li> <li><code>npm run dev:serve</code> to start the server responding to API requests.</li> <li><code>npm run build &amp;&amp; npm run plot</code> to plot some of the analysis after a pipeline has run</li> </ul>"},{"location":"#troubleshooting","title":"Troubleshooting","text":""},{"location":"#general-tips","title":"General tips","text":"<p>There are a few places to look for diagnostics to help you if something has gone wrong e.g. if you received Matrix notifications to the devops channel that the pipeline failed or the server went down.</p> <ul> <li>The <code>property-boundaries-service/logs</code> folder on the server has a record of the logs from past   pipelines. You can identify files by the pipeline ID an its start time, which are in the filename.   There will hopefully be an error log at the end of the file, indicating something went wrong</li> <li>You can also check the last 100 pm2 error logs by running <code>pm2 logs --err --lines 100</code>. These are   the logs that are sent stderr (with <code>console.error</code> in the code)</li> <li>If there still isn't much info, you might find more in the logs for the systemd service that's   running pm2 by running <code>journalctl --user -u pm2.service</code> (note this is specific to our DCC deployment).   Often this is where OOMEs will show up - see below for how to further debug these.</li> <li>If you still don't have any explanation, maybe the whole server rebooted unexpectedly - you can check   this with <code>journalctl -u systemd-shutdown</code> (these last up to a month), or <code>last reboot</code>.</li> </ul>"},{"location":"#known-errors","title":"Known errors","text":"<p>Please add to this list as you encounter problems during development.</p>"},{"location":"#pipeline-fails-with-unable-to-find-driver-mysql-error","title":"Pipeline fails with \"Unable to find driver `MySQL'\" error","text":"<p>In the <code>downloadInspire</code> task of the pipeline, transforming GML data and inserting it into MySQL step doesn't work on MacOS (and maybe Windows?) since the GDAL ogr2ogr MySQL driver isn't supported. I couldn't work out how to get this to work, but it works find on the Linux install. Therefore, testing of this part of the pipeline must be done on the dev server.</p> <p>You can dump (some of) the <code>pending_inspire_polygons</code> DB table from the server and import this on your local machine in order to test the <code>analyseInspire</code> task locally.</p>"},{"location":"#out-of-memory-errors-oomes","title":"Out of memory errors (OOMEs)","text":"<p>There's a helpful guide here on how to assess memory leaks in Node https://stackoverflow.com/a/66563755</p> <p>If you stop the app (<code>pm2 stop property-boundaries-service</code>) and restart it with <code>npm run debug:serve</code>, the app will run with the <code>--heapsnapshot-signal</code> and <code>--heapsnapshot-near-heap-limit</code> Node arguments. This will try to produce a heap dump when the heap usage is approaching the limit, or it will write a dump whenever you send a signal, using <code>npm run debug:dump</code>. You can monitor the current heap usage with <code>pm2 monit</code>. When served in debug mode, the Node app also has a reduced max heap limit, so the limit is reached sooner, and it's more likely to be able to write the dumps.</p> <p>You can then download these dump files to your local machine and view them in your browser's dev tools.</p>"},{"location":"#the-whole-database-has-been-messed-up-by-some-bad-pipeline-code","title":"The whole database has been messed up by some bad pipeline code \ud83d\ude31","text":"<p>Ideally, this should only happen on dev-2 where you're testing new code, but may happen on staging or prod too.</p> <p>Our Borg backup process automatically creates archives each month of the prod-2 database and stores a copy on the Hetzner storage box and also locally on each of prod-2 (itself), staging-2 and dev-2.</p> <p>You can rollback the database to the state of this latest archive by running the following script on the server (as the PBS app user):</p> <pre><code>screen -m -d -S rollback-property_boundaries ~/.local/bin/borgmatic/rollback-property_boundaries.sh &amp;\n</code></pre> <p>The <code>screen</code> command is used to run the rollback in a detached screen session since it may take a while to complete.</p>"},{"location":"database/","title":"The Database","text":"<p>We use MySQL, currently on version 8.0.41.</p> <p>There are the tables in our database:</p> <ul> <li><code>land_ownerships</code> -   each row is a title register i.e. the ownership info of a freehold and leasehold</li> <li><code>land_ownership_polygons</code> -   each row gives the geometry of a land boundary for a registered property polygon, and possibly a   title no, which will link it to the above table with a many-to-one relationship (polygons-to-title).</li> <li><code>pipeline-runs</code> -   each row provides details of a run of the pipeline, indicating its start time, status and progress.</li> <li><code>pending-inspire-polygons</code> -    this is where pending geometries of newly downloaded INSPIRE polys are stored, then each one is   analysed by the pipeline algorithm and marked as accepted or failed. Eventually they are written   into the main <code>land_ownerships</code> table</li> <li><code>pending-polygon-deletions</code> -   a list of INSPIRE IDs that are marked for deletion by the pipeline algorithm. Currently this is not   used, but would be used if e.g. 2 INSPIRE polygons have merged and we want to delete one of the old   polygons that no longer exists.</li> <li><code>unregistered_land</code> -   each row has the geometry of a polygon boundary for a piece of unregistered land</li> <li><code>england_and_wales</code> -   this is a temporary table, created once for one-time unregistered land initialisation script, so   doesn't have a Sequelize migration</li> <li><code>os_land_polys</code> -   this is a table used in the one-time unregistered land initialisation script</li> </ul> <p>View the <code>migrations</code> folder to see their full definitions.</p> <p>You can find more background on how the data relates to the real world in pipeline.md.</p>"},{"location":"database/#backups","title":"Backups","text":"<p>At DCC, we use Borgmatic (powered by Borg) to handle our database backups. This is configured by our DCC Ansible scripts.</p> <p>A backup of the production database is scheduled to run on the 8th night of each month (to ensure it's after the month's INSPIRE data has been published on the first Sunday of the month). Borg archives are kept in a remote storage box, and also the latest archive is stored locally on each server (prod-2, staging-2 and dev-2) for ease of restoration in an emergency.</p> <p>To avoid getting into too many details here that are specific to our DCC infrastructure, see this GitHub comment for the full picture.</p>"},{"location":"database/#how-to-restore","title":"How to restore","text":"<p>On any of prod-2, staging-2 and dev-2, to rollback the database to the latest Borg archive, which is kept locally on each server, you can run <code>npm run rollback-db</code>. This will run the script that our Ansible setup scripts should have installed in <code>~/.local/bin/borgmatic/rollback-$DB_NAME.sh</code>.</p> <p>If you are using this application in a non-DCC setup, you can save your own <code>rollback-$DB_NAME.sh</code> script to this location.</p>"},{"location":"deployment/","title":"DCC servers","text":"<p>We have a dev, staging, and prod version of the PBS hosted on Hetzner servers dev-2, staging-2, and prod-2.</p> <p>The apps are deployed to https://dev.propertyboundaries.landexplorer.coop/, https://staging.propertyboundaries.landexplorer.coop/, and https://propertyboundaries.landexplorer.coop/.</p> <p>Generally, we deploy the <code>development</code> branch to dev-2, where we can test new features/fixes that are in development, and the <code>main</code> branch to staging-2 for QA... and then finally also to prod-2.</p> <p>See more details in this GitHub comment.</p>"},{"location":"deployment/#deployment-instructions","title":"Deployment Instructions","text":""},{"location":"deployment/#requirements","title":"Requirements","text":"<ul> <li>NodeJS</li> <li>MySQL</li> <li>GDAL tools (includes the <code>ogr2ogr</code> command line tool)</li> <li>PM2</li> </ul>"},{"location":"deployment/#first-install","title":"First install","text":"<ol> <li>Set up requirements on the remote machine you want to deply the PBS on. At DCC, we do this by running an Ansible playbook (see technology-and-infrastructure)</li> <li>Run the <code>install-remote.sh</code> script from your local machine to install the application on the desired remote user@hostname. e.g.:</li> </ol> <pre><code>bash install-remote.sh -u aubergine root@prod-2.digitalcommons.coop\n</code></pre> <p>Note: that this will only succeed once you have uploaded its public SSH key to GitHub SSH (explained in the script's output).</p> <ol> <li>Log into the server and, in the codebase, copy <code>.env.example</code> to <code>.env</code>.</li> <li>Fill in <code>.env</code> with the credentials and API keys (in BitWarden or the password-store).</li> <li><code>bash scripts/deploy.sh</code> to install dependencies, run the DB migration scripts, build and serve the app with PM2</li> </ol>"},{"location":"deployment/#subsequent-updates","title":"Subsequent updates","text":"<p>Checkout the code that you wish to deploy then <code>bash scripts/deploy.sh</code>.</p>"},{"location":"pipeline/","title":"The Ownerships + INSPIRE updates pipeline","text":"<p>The purpose of the Ownerships + INSPIRE pipeline is to retrieve new data from open source government datasets and use it to update the data in our PBS database, which can then be served to Land Explorer.</p> <p>Note that we currently only have property data for England and Wales.</p> <p>All of the pipeline-related code lives in the <code>src/pipeline</code> directory.</p>"},{"location":"pipeline/#overview-of-the-data","title":"Overview of the data","text":"<p>We store data in 3 database tables:</p> <ul> <li><code>land_ownerships</code> -   each row in this table links a 'title' (i.e. a title deed for a single property in the Land Registry)   to a company that owns it. It includes the 'tenure' (leasehold or freehold), when the title was added to the Land Registry, and various details about the company.</li> <li><code>land_ownership_polygons</code> -   each row in this table gives the geometry of a land boundary for a registered property. It has a <code>poly_id</code> (a.k.a. INSPIRE ID) and possibly a <code>title_no</code> to link the property to on ownership in the above table.</li> <li><code>unregistered_land</code> -   each row has the geometry of a polygon boundary for a piece of unregistered land.</li> </ul> <p>The pipeline obtains data from these open source datasets:</p> <ul> <li> <p>INSPIRE - a set of polygons, purely land boundaries of freehold properties (but maybe not a complete set?). Each has an INSPIRE ID. This data set does not link Title Number to these polygons. An updated full dataset is published each month, and there is no way   to access historical data.</p> </li> <li> <p>Land Reg UK Companies - A list of all UK companies that own property, linking a company to a Title Number.</p> </li> <li> <p>Land Reg Overseas Companies - A list of overseas companies that own property, linking a company to a Title Number.</p> </li> </ul> <p>In the past, we also used the following closed source dataset:</p> <ul> <li>National Polygon Service (closed source) - Boundaries for leaseholds AND freeholds, plus Title Numbers for MOST of these freeholds/leaseholds (the spec says all should have matched titles but some don\u2019t seem to have them). A Title Number is unique to that property and doesn't change between owners. Also, a single title may have multiple linked polygons with separate poly_ids e.g. when garages are in blocks separate from housing.</li> </ul> <p>Note: We refer to poly_id and INSPIRE ID interchangeably because they are the same. The set of INSPIRE polygons is a subset of all polygons in the National Polygon Service, and they share the same ID in each dataset. See the dataset's technical specification for more info.</p>"},{"location":"pipeline/#the-crux-of-the-issue","title":"The crux of the issue","text":"<p>We gained a copy of the National Polygon Service for evaluation purposes a few years back, but no longer have access. This means there is no longer a way to reliably link Titles to INSPIRE boundary polygons. The purpose of the pipeline is therefore to attempt to update ownership and boundary data, whilst preserving the link between titles and polygons where possible.</p>"},{"location":"pipeline/#different-cases-of-data-changing","title":"Different cases of data changing","text":"<p>In order to achive the above purpose, we need to understand how &amp; why the data might change, so that we can recognise these cases and know how to procede in our pipeline.</p> <p>Here is a list of cases (W.I.P.) and what we do in the pipeline for each scenario:</p> <ol> <li> <p>The ownership of a title changes in one of the company ownership datasets (UK or overseas)</p> <ul> <li>Since a Title Number is unique to that property and shouldn't change between owners, we can   assume that any polygons linked to that title are now owned by the new company. So we just   need to update the title in the <code>land_ownerships</code> table and make no changes to <code>land_ownership_polygons</code>.</li> </ul> </li> <li> <p>A freehold title is removed from the company ownership datasets</p> <ul> <li>If the title's linked INPSIRE polygon(s) have merged with an adjacent polygon, see case 7 below. Otherwise:</li> <li>This indicates that the title has been sold to a private individual. If the title had linked polygon(s) which are unchanged, we can keep the link to the title number. There's a chance it will be sold to a company in the future, so more ownership info will be visible again. So remove the title from the <code>land_ownerships</code> table and make no changes to <code>land_ownership_polygons</code>.</li> </ul> </li> <li> <p>A leasehold title is removed from the company ownership datasets</p> <ul> <li>EITHER the title has been sold to a private individual, in which case we can keep the link   between title number and polygon(s). So remove the title from the <code>land_ownerships</code> table and make no changes to <code>land_ownership_polygons</code>.</li> <li>OR the lease may have been closed by a merger if the same company also owns the freehold. We don't have enough info to tell if this happened, but may be able to use the Registered Leases or Price Paid Data datasets in the future to help with this. For now, just assume it was the former scenario.</li> </ul> </li> <li> <p>An INSPIRE polygon's boundary changes very slightly</p> <ul> <li>This indicates that there has been a new survey by the local authority and any freehold title will still be linked to that property. Or it also occured when we improved our coordinate transformation when importing the INSPIRE data. So update the coordinates in <code>land_ownership_polygons</code>.</li> <li>(TODO) Since INSPIRE doesn't include leasehold polygons, we should also alter any associated leashold boundaries. This isn't implemented yet. We also need to fix the leashold boudaries that were left using the old coordinate transformation.</li> </ul> </li> <li> <p>An INSPIRE polygon's boundary moves by a large distance</p> <ul> <li>This is unexpected, since usually a new INSPIRE ID would just be made. We should examine these instances manually (TODO). If the polygon has an associated company-owned title, we can geocode the title's address and check whether the new location matches.</li> </ul> </li> <li> <p>An INSPIRE polygon splits into two or more parts</p> <ul> <li>When freehold titles are split, the owner will usually be selling off a portion of the property (otherwise it usually makes more sense for them to split into leaseholds). More info here and here. The portion that they keep will retain the old Title Number and the new segment of land will be assigned a new Title Number. So maybe when this happens, we can check for new company-owned titles with the same or adjacent address and link them (TODO).</li> </ul> </li> <li> <p>Two or more INSPIRE polygons merge</p> <ul> <li>Freeholds can be amalgamated if they're owned by the same proprietor. Usually, the largest property's Title Number will be chosen for the new amalgamated title - see section 14.7.1 of this guide. We can try to cross-reference with the company-owned titles data to see if this is the case. If the titles were company-owned, we'll hopefully see that all amalgamated titles apart from one are removed from the dataset, and that the old ones had the same proprietor (TODO).</li> </ul> </li> </ol>"},{"location":"pipeline/#a-big-limitation-leaseholds","title":"A big limitation: leaseholds","text":"<p>See https://github.com/DigitalCommons/property-boundaries-service/issues/26</p>"},{"location":"pipeline/#stages-of-the-pipeline","title":"Stages of the pipeline","text":"<p>The main function that runs the pipeline is <code>runPipeline()</code> in run.ts.</p> <p>It runs these tasks in sequential order:</p> <ol> <li> <p><code>ownerships</code>: This task updates the <code>land_ownerships</code> table (see above), using the latest company     ownership data. This stage is quite fast and non-destructive, since the government API provides all     historical data since Nov 2017, so the new data is always written straight into the DB.</p> </li> <li> <p><code>downloadInspire</code>: This task</p> <ol> <li>downloads the latest INSPIRE data</li> <li>backs up this data to our Hetzner storage box</li> <li>unzips the downloaded GML data then, using GDAL, transforms it to the standard EPSG:4326    coordinate system and inserts all polygons into the <code>pending_inspire_polygons</code> DB table</li> </ol> </li> <li> <p><code>analyseInspire</code>: This task does the following steps:</p> <ol> <li> <p>Loop one-by-one through <code>pending_inspire_polygons</code>. Check if the poly_id already exists in our   <code>land_ownership_polygons</code> table.</p> <ul> <li> <p>If so, we compare the old and new polys and try to classify a match. i.e. one of the scenarios in the above section. In some cases, the algorithm fails to classify the match, maybe because the polyon changed in an unexpected way. You can see descriptions of the match types implemented so far in <code>match.ts</code>. Depending on how the match is classified, we mark pending polygons as 'accepted' or 'rejected', and maybe mark existing polygons to be deleted.</p> </li> <li> <p>If the poly_id is new, we check that it's not overlapping with an existing polygon in our DB. If it isn't, we mark the new poly as accepted. We have some rough 'amalgamation' detection code to cover more cases but haven't implemented it yet.</p> </li> </ul> <p>Note: The plan is to improve these algorithms over time, based on manual review of our pipeline's results and research of different scenarios (in the above section). So it will eventually identify and classify more scenarios, rather than just marking them as failed matches.</p> </li> <li> <p>If the <code>updateBoundaries</code> pipeline option was set to true, clip the boundaries of all new or    changed<code>pending_inspire_polygons</code> (registered freehold boundaries) from the    <code>unregistered_land</code> table, write all accepted <code>pending_inspire_polygons</code> into    <code>land_ownership_polygons</code> (overwriting existing geometry data) and delete all polygons listed    in <code>pending_polygon_deletions</code>.</p> </li> </ol> </li> </ol>"},{"location":"pipeline/#how-to-run-it","title":"How to run it","text":""},{"location":"pipeline/#manually","title":"Manually","text":"<p>The pipeline can be triggered by an API request like this:</p> <pre><code>https://&lt;PBS URL&gt;/run-pipeline?secret=&lt;secret&gt;\n</code></pre> <p>The pipeline can be started with additional options (see <code>PipelineOptions</code> in run.ts for details), e.g.:</p> <pre><code>https://&lt;PBS URL&gt;/run-pipeline?secret=&lt;secret&gt;&amp;startAtTask=analyseInspire&amp;maxCouncils=5\n</code></pre> <p>Note: The new INSPIRE boundaries will remain in the <code>pending_inspire_polygons</code> table where they can be manually analysed (see below) and will not be written into the actual <code>land_ownership_polygons</code> table unless the <code>updateBoundaries</code> pipeline option is set to true.</p>"},{"location":"pipeline/#automatic-runs","title":"Automatic runs","text":"<p>At DCC, we have automatic scripts to hit the API described above, so that the pipeline runs automatically.</p> <p>They are scheduled to run after a Borg backup of the production database has been completed, which itself is scheduled to run on the 8th night of each month (to ensure it's after the month's INSPIRE data has been published on the first Sunday of the month). To avoid getting into too many details that are specific to DCC infrastructure, see this GitHub comment for more details, and deployment.md for a rough overview of our DCC deployment.</p>"},{"location":"pipeline/#analysing-the-pipeline-output","title":"Analysing the pipeline output","text":"<p>After the <code>updateOwnerships</code> task is complete, the new company ownership data should be visible in LX for all users.</p> <p>After the <code>downloadOwnerships</code> task, a LX super user can see the pending INSPIRE polygons that have been downloaded in a separate, secret data layer.</p> <p>After the <code>analyseInspire</code> task has been run, the pending INSPIRE polygons will be marked 'accepted' if the matching algorithm thinks they are ready to be inserted into the main <code>land_ownership_polygons</code> DB table (either as a new row, or updating the geometry of an existing row if it has the same <code>poly_id</code>). If a pending polygon has been marked as 'accepted', this will be visible in the secret data layer on LX as a green boundary. Non-accepted polys will appear as yellow.</p> <p>Further detailed output for the pipeline can be found in the <code>analysis</code> folder in the project's root folder. This output can help you manually investigate something further e.g. if you want to investigate a failed match:</p> <ul> <li>find the details in <code>failed-matches.json</code> and copy a lng-lat of a vertex</li> <li>login to LX as a super user (to become a super user, update your record in MySQL on the server)</li> <li>enable the Pending Polygons data layer</li> <li>search for the lng-lat in the LX search bar, then click on the properties in view to find and the polygon(s) involved</li> </ul> <p>Once you are satisfied that the <code>analyseInspire</code> task has marked the correct polygons as 'accepted', you can trigger the following API request to write them into the main DB table:</p> <pre><code>https://&lt;PBS URL&gt;/run-pipeline?secret=&lt;secret&gt;&amp;resume=true&amp;updateBoundaries=true\n</code></pre>"},{"location":"testing/","title":"Testing","text":""},{"location":"testing/#unit-tests","title":"Unit tests","text":"<p>To run all UTs, run <code>npm run test</code>.</p>"},{"location":"todo/","title":"TODO","text":"<p>(Roughly in priority order)</p> <p>Remove items from this file once a GitHub issues are created for them.</p> <ul> <li> <p>Fully spec the desired behaviour of the pipeline, in particular the matching algorithm for INSPIRE polygons, and add unit tests to match this spec</p> <ul> <li>Mocha is set up for this. I made a methods.test.ts file, using GitHub Copilot, inspired by the Land Explorer backend, with UTs for the currently implemented matching algorithm.</li> <li>We'll probably need to eventually modularise some of the long functions in the pipeline (e.g. the <code>comparePolygons</code> function) to make unit testing easier, and this will make the code more understandable too.</li> <li>Maybe we need to plot some different polygon scenarios that can be visualised and used for different test cases in UTs.</li> </ul> </li> <li> <p>Once we have the above spec, systematically address parts of the pipeline that we want to improve. This will involve:</p> <ol> <li>manually looking at the most common failed matches</li> <li>adding to the research of cases in pipeline.md and identifying gaps in our algorithm</li> <li>stepping back to think about who will be using the app and which matches are most valuable for us to prioritise in order to preserve     as much useful data as we can. Think about whether we need any other data sources to achieve this</li> </ol> </li> <li> <p>Add analytics and do profiling to find where the bottlenecks are in analysis script, so they can be optimised. For each council, the script currently takes about 5 mins to download and transform the data and 5 mins to do a simply analysis of all the polygons (skipping more complicated analysis of segmentations/merges). Tasks could maybe be parallelised using worker threads (see https://nodejs.org/api/worker_threads.html). Also decide which bits of the algorithm are most needed and remove some computation that isn't necessary. And allow pipelines to resume automatically if something goes wrong e.g. the server reboots, which is fairly likely since the pipeline is going to take a long time even if we optimise it really well (it's processing a huge amount of data!). We could maybe use Glitchtip (which we use for MM) for error and performamce tracking.</p> </li> <li> <p>Improve how the results of the analysis can be understood/visualised. It's currently a lot of data, and it's hard to know which matches to check individually.</p> </li> <li> <p>Enable strict Typescript checking in tsconfig.json and fixup existing checking failures. Use more modern Sequelize definitions so that we get types https://sequelize.org/docs/v7/models/defining-models/</p> </li> <li> <p>Address the various 'TODO' comments around the codebase</p> </li> </ul>"}]}